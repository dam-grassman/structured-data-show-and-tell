# Show and tell: A neural image caption generator

As part of the course **Structured Data: Learning, Prediction Dependency, Testing** (M2 Data Science - Universit√© Paris Saclay)

Online Notebook viewer : 
https://nbviewer.jupyter.org/github/dam-grassman/structured-data-show-and-tell/blob/master/Structured-Data-Show-and-Tell.ipynb

## Introduction 

Automatic captioning of images constitutes a widely studied domain in the computer vision
community. Both the connection it makes between computer vision and natural language
processing and its wide range of possible applications, including help for visually impaired
people, make it a hot topic in artificial intelligence.

In this github (Report & Notebook), we present and discuss
the paper *Show and tell: a neural image caption generator* by O. Vinyals, A. Toshev,
S. Bengio and D. Erhan in 2014 [1] in which the authors study that problem.

We finally implement step by step in the notebook, the proposed model using the Flickr8k dataset.

## Outline 

### 1. Description and Understanding of the Flickr8k Dataset

### 2.  A Neural Image Caption Generator
 - 2.1 Image Encoder using Pre-Trained CNN models
 -   2.2 LSTM-based Sentence Generator
 -   2.3 Model : Mixing all together
  
### 3. Image Captioning Results
 - 3.1 Decoding Strategies
 - 3.2 Examples
 -   3.3 Analysis
